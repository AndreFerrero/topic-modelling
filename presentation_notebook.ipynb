{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project overview\n",
    "\n",
    "The goal of the project is to evaluate how different models perform in the task of topic modelling.\n",
    "The models used by the paper are:\n",
    "1. Latent Dirchlet Allocation (LDA)\n",
    "2. Latent Semantic Analysis (LSA)\n",
    "3. Non-Negative Matrix Factorization (NMF)\n",
    "4. Principal Component Analysis (PCA)\n",
    "5. Random Projection (RP)\n",
    "\n",
    "The evaluation performed by the authors consisted in examining models performances when changing the **number of topics** or the **number of words**, keeping all the others parameters fixed.\n",
    "\n",
    "The authors didn't specify which parameters were fixed while performing evaluation, therefore we had to choose our own settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importing and preprocessing\n",
    "The data used comes from the 20-Newsgroups dataset, available online, containing various texts from different topics. The importing required preprocessing steps such as:\n",
    "- removal of special characters using regular expressions\n",
    "- tokenization\n",
    "- lemmatization with the aid of Part Of Speech\n",
    "\n",
    "Below the code used for importing and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import os\n",
    "import re, string\n",
    "from nltk.corpus import words, wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def import_data(path):\n",
    "    \n",
    "    folders_path_list = []\n",
    "    folders = os.listdir(path)\n",
    "    \n",
    "    for folder in folders:\n",
    "        folders_path_list.append(os.path.join(path, folder))\n",
    "    \n",
    "    conversion = {'religion' : [folders[i] for i in [0, 15, 19]],\n",
    "                  'computer': [folders[i] for i in range(1, 6)],\n",
    "                  'science': [folders[i] for i in range(11, 15)],\n",
    "                  'politics': [folders[i] for i in range(16, 19)],\n",
    "                  'misc': [folders[6]],\n",
    "                  'recreation': [folders[i] for i in range(7, 11)]}\n",
    "    \n",
    "    topics = folders.copy()\n",
    "    \n",
    "    for j, folder in enumerate(folders):\n",
    "        for topic, values in conversion.items():\n",
    "            \n",
    "            if folder in values:\n",
    "                topics[j] = topic\n",
    "    \n",
    "    \n",
    "    column_names = ['File_Name', 'Content', 'Folder', 'Topic']\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    for folder, folder_path, topic in zip(folders, folders_path_list, topics):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                content = file.read()\n",
    "                df = pd.concat([df,\n",
    "                                pd.DataFrame({'File_Name': [file_name], 'Content': [content],\n",
    "                                              'Folder' : [folder], 'Topic': [topic]})],\n",
    "                               ignore_index=True)\n",
    "                \n",
    "    df['Content'] = df['Content'].astype(\"string\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def preprocess(doc):\n",
    "    \n",
    "    # Remove email addresses\n",
    "    doc = re.sub(r'\\b\\S*@\\S*\\.\\S*\\b', '', doc)\n",
    "    \n",
    "    # Remove special characters and digits, retain only words with letters\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "    \n",
    "    # Lowercase and strip\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # Remove brackets of any kind\n",
    "    doc = re.sub(r'[(){}[\\]]', '', doc)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    doc = doc.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Tokenize document\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    # POS tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "     # map POS tags to WordNet POS tags\n",
    "    tag_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # .get() returns value associated to keyname. If keyname is not a key, it returns what's specified in value\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, tag_map.get(pos[0], wordnet.NOUN)) for token, pos in pos_tags]\n",
    "    \n",
    "    \n",
    "    # Filter stopwords out of lemmatized tokens\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    stop_words.extend(['hi', 'thanks', 'lot', 'dont', 'article', 'everyone', 'anyone',\n",
    "                       'someone', 'nothing',\n",
    "                       'something', 'anything', 'everybody', 'somebody', 'anybody',\n",
    "                       'please', 'ask', 'people', 'university',\n",
    "                       'question', 'yeah', 'shouldnt', 'theyre', 'thing', 'theyll', 'didnt', 'sorry', 'hey',\n",
    "                       'oh', 'thats', 'thank', 'cannot', 'right', 'would', 'one', 'get', 'know', 'like', 'use', 'go',\n",
    "                       'think', 'make', 'say', 'see', 'also', 'could', 'well', 'want', 'way', 'take', 'find', 'need', 'try',\n",
    "                       'much', 'come', 'many', 'may', 'give', 'really', 'tell', 'two', 'still', 'read', 'might', 'write',\n",
    "                       'never', 'look', 'sure', 'day', 'even', 'new', 'time', 'good', 'first', 'keep', 'since', 'last', \n",
    "                       'long', 'fact', 'must', 'cant', 'another', 'little', 'without', 'csutexasedu', 'nntppostinghost',\n",
    "                       'im', 'seem', 'replyto', 'let', 'group', 'call', 'seem', ])\n",
    "    \n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    \n",
    "    # Recreate the document\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "path = R\"~\\path\"\n",
    "\n",
    "df = import_data(path)\n",
    "\n",
    "df['Clean_Content'] = df['Content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TFIDF and other objects\n",
    "Once the dataset has been imported and preprocessed, a TFIDF matrix was constructed as input matrix for the models. To do this, it was convenient to write another function that could be called inside the notebook. Besides the tfidf matrix, given a specific set of parameters, the function also returned some objects required for topic modelling with the gensim package, which is the one used for some of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, matutils\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_tfidf_tokendocs_corpus_dict(df, max_df, min_df, max_features): #0.5, 5, 5000\n",
    "    # convert text into lists\n",
    "    documents = df['Clean_Content'].tolist()\n",
    "    documents\n",
    "\n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, min_df = min_df, norm = 'l2', max_features=max_features)\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # this is a list of documents with tokens\n",
    "    # it's needed for the coherence function\n",
    "    tokenized_docs = [word_tokenize(document) for document in documents]\n",
    "    \n",
    "    # Convert TF-IDF matrix to Gensim corpus\n",
    "    corpus = matutils.Sparse2Corpus(tfidf_matrix.transpose())\n",
    "    # Convert the document-term matrix to a gensim Dictionary\n",
    "    dictionary = corpora.Dictionary.from_corpus(corpus,\n",
    "                                            id2word=dict((id, word) for word, id in vectorizer.vocabulary_.items()))\n",
    "    \n",
    "    return tfidf_matrix, feature_names, tokenized_docs, corpus, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
