{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project overview\n",
    "\n",
    "The goal of the project is to evaluate how different models perform in the task of topic modelling.\n",
    "The models used by the auuthors of the paper are:\n",
    "1. Latent Dirchlet Allocation (LDA)\n",
    "2. Latent Semantic Analysis (LSA)\n",
    "3. Non-Negative Matrix Factorization (NMF)\n",
    "4. Principal Component Analysis (PCA)\n",
    "5. Random Projection (RP)\n",
    "\n",
    "The evaluation performed by the authors consisted in examining models performances when changing the **number of topics** or the **number of words**, keeping all the others parameters fixed.\n",
    "\n",
    "The authors didn't specify which parameters were fixed while performing evaluation, therefore we had to choose our own settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importing and preprocessing\n",
    "The data used comes from the 20-Newsgroups dataset, available online, containing various texts from different topics. The data required preprocessing steps such as:\n",
    "- removal of special characters using regular expressions\n",
    "- tokenization\n",
    "- lemmatization with the aid of Part Of Speech\n",
    "\n",
    "Below the code used for importing and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import os\n",
    "import re, string\n",
    "from nltk.corpus import words, wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def import_data(path):\n",
    "    data_path = R\"data\\data-train\"\n",
    "    full_path = os.path.join(path, data_path)\n",
    "    \n",
    "    folders_path_list = []\n",
    "    folders = os.listdir(full_path)\n",
    "    \n",
    "    for folder in folders:\n",
    "        folders_path_list.append(os.path.join(full_path, folder))\n",
    "    \n",
    "    conversion = {'religion' : [folders[i] for i in [0, 15, 19]],\n",
    "                  'computer': [folders[i] for i in range(1, 6)],\n",
    "                  'science': [folders[i] for i in range(11, 15)],\n",
    "                  'politics': [folders[i] for i in range(16, 19)],\n",
    "                  'misc': [folders[6]],\n",
    "                  'recreation': [folders[i] for i in range(7, 11)]}\n",
    "    \n",
    "    topics = folders.copy()\n",
    "    \n",
    "    for j, folder in enumerate(folders):\n",
    "        for topic, values in conversion.items():\n",
    "            \n",
    "            if folder in values:\n",
    "                topics[j] = topic\n",
    "    \n",
    "    \n",
    "    column_names = ['File_Name', 'Content', 'Folder', 'Topic']\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    for folder, folder_path, topic in zip(folders, folders_path_list, topics):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                content = file.read()\n",
    "                df = pd.concat([df,\n",
    "                                pd.DataFrame({'File_Name': [file_name], 'Content': [content],\n",
    "                                              'Folder' : [folder], 'Topic': [topic]})],\n",
    "                               ignore_index=True)\n",
    "                \n",
    "    df['Content'] = df['Content'].astype(\"string\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def preprocess(doc):\n",
    "    # Tokenize document\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    # Expand contractions\n",
    "    tokens = [contractions.fix(token) for token in tokens]\n",
    "    \n",
    "    # Rejoin tokens into a string\n",
    "    doc = ' '.join(tokens)\n",
    "    \n",
    "    # Remove special characters, retain only words with letters\n",
    "    doc = re.sub(r'[^\\w\\s\\']', '', doc)\n",
    "    \n",
    "    # remove digits\n",
    "    doc = re.sub(r'[0-9]+', '', doc)\n",
    "    \n",
    "    # Remove brackets of any kind\n",
    "    doc = re.sub(r'[(){}[\\]]', '', doc)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    doc = doc.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Lowercase and strip\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # Tokenize again after cleaning\n",
    "    cleaned_tokens = word_tokenize(doc)\n",
    "\n",
    "    # POS tagging on the original tokenized version\n",
    "    pos_tags = pos_tag(cleaned_tokens)\n",
    "    \n",
    "    # map POS tags to WordNet POS tags\n",
    "    tag_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, tag_map.get(pos[0], wordnet.NOUN)) for token, pos in pos_tags]\n",
    "    \n",
    "    # Filter stopwords out of lemmatized tokens\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    stop_words.extend(['hi', 'thanks', 'lot', 'article', 'everyone',\n",
    "                       'anyone', 'someone', 'nothing',\n",
    "                       'something', 'anything', 'everybody', 'somebody', 'anybody',\n",
    "                       'please', 'ask', 'people', 'university',\n",
    "                       'question', 'yeah', 'thing', 'sorry', 'hey', 'oh',\n",
    "                       'thank', 'cannot', 'right', 'would', 'one', 'get',\n",
    "                       'know', 'like', 'use', 'go',\n",
    "                       'think', 'make', 'say', 'see', 'also', 'could', 'well', 'want',\n",
    "                       'way', 'take', 'find', 'need', 'try',\n",
    "                       'much', 'come', 'many', 'may', 'give', 'really', 'tell',\n",
    "                       'two', 'still', 'read', 'might', 'write',\n",
    "                       'never', 'look', 'sure', 'day', 'even', 'new', 'time',\n",
    "                       'good', 'first', 'keep', 'since', 'last', \n",
    "                       'long', 'fact', 'must', 'cant', 'another', 'little',\n",
    "                       'without', 'csutexasedu', 'nntppostinghost',\n",
    "                       'seem', 'replyto', 'let', 'group', 'call', 'seem',\n",
    "                       'maybe','shall', 'eg', 'etc', 'rather', 'either'])\n",
    "    \n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    \n",
    "    # Recreate the document\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "scripts_path = os.getcwd()\n",
    "path = os.path.dirname(scripts_path)\n",
    "\n",
    "df = import_data(path)\n",
    "\n",
    "df['Clean_Content'] = df['Content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TFIDF and other objects\n",
    "Once the dataset has been imported and preprocessed, a TFIDF matrix was constructed as input matrix for the models. To do this, it was convenient to write another function that could be called inside the notebook. Besides the tfidf matrix, given a specific set of parameters, the function also returned some objects required for topic modelling with the gensim package, which is the one used for some of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, matutils\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_tfidf_tokendocs_corpus_dict(df, max_df, min_df, max_features):\n",
    "    # convert text into lists\n",
    "    documents = df['Clean_Content'].tolist()\n",
    "    documents\n",
    "\n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, min_df = min_df, norm = 'l2', max_features=max_features)\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # this is a list of documents with tokens\n",
    "    # it's needed for the coherence function\n",
    "    tokenized_docs = [word_tokenize(document) for document in documents]\n",
    "    \n",
    "    # Convert TF-IDF matrix to Gensim corpus\n",
    "    corpus = matutils.Sparse2Corpus(tfidf_matrix.transpose())\n",
    "    # Convert the document-term matrix to a gensim Dictionary\n",
    "    dictionary = corpora.Dictionary.from_corpus(corpus,\n",
    "                                            id2word=dict((id, word) for word, id in vectorizer.vocabulary_.items()))\n",
    "    \n",
    "    return tfidf_matrix, feature_names, tokenized_docs, corpus, dictionary\n",
    "\n",
    "\n",
    "tfidf_matrix, feature_names, tokenized_docs, corpus, dictionary = get_tfidf_tokendocs_corpus_dict(df, max_df=0.5, min_df=5, max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models evaluation\n",
    "The evaluation of the models was done by comparing Coherence, a measure of topics quality. We replicated the analysis of the paper by comparing the goodness of the models for different number of topics (5, 10, 20, 50) and different number of words (10, 100, 1000, 10000).\n",
    "\n",
    "To perform the evaluation, we chose to write functions that ran the models different times for the specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "from tfidf_corpus_dictionary import get_tfidf_tokendocs_corpus_dict\n",
    "from gensim.models import LdaModel, LsiModel, CoherenceModel\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "topics = [5, 10, 20, 50]\n",
    "words = [10, 100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation by topics:\n",
    "```python\n",
    "def coherence_by_topics(n: int, corpus, dictionary, texts, feature_names, tfidf):\n",
    "    models = ['LDA', 'LSA', 'NMF', 'PCA', 'RP']\n",
    "\n",
    "    coherence = []\n",
    "\n",
    "    for model_name in models:\n",
    "        if model_name == 'LDA' or model_name == 'LSA':\n",
    "            \n",
    "            if model_name == 'LDA':\n",
    "                model = LdaModel(corpus=corpus, id2word=dictionary,\n",
    "                                num_topics = n,\n",
    "                                alpha='symmetric', eta='auto', passes=5,\n",
    "                                random_state=1)\n",
    "        \n",
    "            elif model_name == 'LSA':\n",
    "                model = LsiModel(corpus, id2word=dictionary,\n",
    "                                num_topics=n, random_seed=1)\n",
    "            \n",
    "            coherence_value = CoherenceModel(model=model,\n",
    "                                            dictionary = dictionary,\n",
    "                                            texts=texts,\n",
    "                                            coherence='c_v').get_coherence()\n",
    "            coherence.append(coherence_value)\n",
    "\n",
    "        elif model_name == 'NMF' or model_name == 'PCA' or model_name == 'RP':\n",
    "            \n",
    "            if model_name == 'NMF':\n",
    "                \n",
    "                model = NMF(n_components=n,\n",
    "                            random_state=1, max_iter=600).fit(tfidf)\n",
    "                \n",
    "            elif model_name == 'RP':\n",
    "                \n",
    "                model = GaussianRandomProjection(n_components=n,\n",
    "                                                random_state=1).fit(tfidf)\n",
    "                \n",
    "            elif model_name == 'PCA':\n",
    "                # Convert sparse matrix to dense. PCA cannot be done on sparse matrixes\n",
    "                tfidf_matrix_dense = tfidf.todense()\n",
    "                            if sparse.issparse(tfidf) else tfidf\n",
    "\n",
    "                # Convert to numpy array\n",
    "                tfidf_matrix_array = np.asarray(tfidf_matrix_dense)\n",
    "\n",
    "                # Centering\n",
    "                mean_tfidf = np.mean(tfidf_matrix_array,\n",
    "                                    axis=0)\n",
    "                centered_tfidf_matrix = tfidf_matrix_array - mean_tfidf\n",
    "                \n",
    "                model = PCA(n_components=n,\n",
    "                            random_state=1).fit(centered_tfidf_matrix)\n",
    "                \n",
    "            # Retrieve top words for each component\n",
    "            topics = []\n",
    "            for j, component in enumerate(model.components_):\n",
    "                component_words = [(feature_names[k], component[k])\n",
    "                                    for k in component.argsort()[::-1]]\n",
    "                topics.append(component_words)\n",
    "                \n",
    "            topics_for_coherence = [[word for word, _ in topic]\n",
    "                                        for topic in topics]\n",
    "            \n",
    "            coherence_value = CoherenceModel(topics=topics_for_coherence,\n",
    "                                            texts=texts, dictionary=dictionary,\n",
    "                                            coherence='c_v').get_coherence()\n",
    "            coherence_value = round(coherence_value, 4)\n",
    "            \n",
    "            coherence.append(coherence_value)\n",
    "    \n",
    "    coherence = [round(num, 4) for num in coherence]\n",
    "\n",
    "    return list(zip(models, coherence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation by words:\n",
    "```python\n",
    "def coherence_by_words(df, n):\n",
    "    models = ['LDA', 'LSA', 'NMF', 'PCA', 'RP']\n",
    "\n",
    "    documents = df['Clean_Content'].tolist()\n",
    "    documents\n",
    "\n",
    "    coherence = []\n",
    "\n",
    "    for model_name in models:\n",
    "        # TF-IDF Vectorization\n",
    "        vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 5,\n",
    "                                    norm = 'l2', max_features=n)\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # this is a list of documents with tokens\n",
    "        # it's needed for the coherence function\n",
    "        texts = [word_tokenize(document) for document in documents]\n",
    "        \n",
    "        # Convert TF-IDF matrix to Gensim corpus\n",
    "        corpus = matutils.Sparse2Corpus(tfidf_matrix.transpose())\n",
    "        # Convert the document-term matrix to a gensim Dictionary\n",
    "        dictionary = corpora.Dictionary.from_corpus(corpus,\n",
    "                                                id2word=dict((id, word)\n",
    "                                                for word, id in\n",
    "                                                vectorizer.vocabulary_.items()))\n",
    "        \n",
    "        if model_name == 'LDA' or model_name == 'LSA':\n",
    "\n",
    "            if model_name == 'LDA':\n",
    "                model = LdaModel(corpus=corpus,\n",
    "                                id2word=dictionary,\n",
    "                                num_topics = 5,\n",
    "                                alpha='symmetric',\n",
    "                                eta='auto', passes=5,\n",
    "                                random_state=1)\n",
    "        \n",
    "            elif model_name == 'LSA':\n",
    "                model = LsiModel(corpus,\n",
    "                                id2word=dictionary,\n",
    "                                num_topics=5,\n",
    "                                random_seed=1)\n",
    "            \n",
    "            coherence_value = CoherenceModel(model=model,\n",
    "                                            dictionary = dictionary,\n",
    "                                            texts=texts,\n",
    "                                            coherence='c_v').get_coherence()\n",
    "            coherence.append(coherence_value)\n",
    "\n",
    "        elif model_name == 'NMF' or model_name == 'PCA' or model_name == 'RP':\n",
    "            \n",
    "            if model_name == 'NMF':\n",
    "                \n",
    "                model = NMF(n_components=5,\n",
    "                            random_state=1,\n",
    "                            max_iter=600).fit(tfidf_matrix)\n",
    "                \n",
    "            elif model_name == 'RP':\n",
    "                \n",
    "                model = GaussianRandomProjection(n_components=5,\n",
    "                                                random_state=1).fit(tfidf_matrix)\n",
    "                \n",
    "            elif model_name == 'PCA':\n",
    "                # Convert sparse matrix to dense. PCA cannot be done on sparse matrixes\n",
    "                tfidf_matrix_dense = tfidf_matrix.todense()\n",
    "                                        if sparse.issparse(tfidf_matrix)\n",
    "                                        else tfidf_matrix\n",
    "\n",
    "                # Convert to numpy array\n",
    "                tfidf_matrix_array = np.asarray(tfidf_matrix_dense)\n",
    "\n",
    "                # Centering with respect to the columns\n",
    "                mean_tfidf = np.mean(tfidf_matrix_array,\n",
    "                                    axis=0)\n",
    "                centered_tfidf_matrix = tfidf_matrix_array - mean_tfidf\n",
    "                \n",
    "                model = PCA(n_components=5,\n",
    "                            random_state=1).fit(centered_tfidf_matrix)\n",
    "                \n",
    "            # Retrieve top words for each component\n",
    "            components = []\n",
    "            for j, component in enumerate(model.components_):\n",
    "                component_words = [(feature_names[k], component[k])\n",
    "                                    for k in component.argsort()[::-1]]\n",
    "                components.append(component_words)\n",
    "                \n",
    "            topics_for_coherence = [[word for word, _ in component]\n",
    "                                        for component in components]\n",
    "            \n",
    "            coherence_value = CoherenceModel(topics=topics_for_coherence,\n",
    "                                            texts=texts,\n",
    "                                            dictionary=dictionary,\n",
    "                                            coherence='c_v').get_coherence()\n",
    "            coherence_value = round(coherence_value, 4)\n",
    "            \n",
    "            coherence.append(coherence_value)\n",
    "    \n",
    "    coherence = [round(num, 4) for num in coherence]\n",
    "        \n",
    "    return list(zip(models, coherence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Storing results\n",
    "```python\n",
    "from coherence_by_topics import coherence_by_topics\n",
    "from coherence_by_words import coherence_by_words\n",
    "\n",
    "evaluation_by_topics = {}\n",
    "\n",
    "for n_topics in topics:\n",
    "    metrics_words = coherence_by_topics(n = n_topics,\n",
    "                                        corpus=corpus,\n",
    "                                        dictionary=dictionary,\n",
    "                                        texts=tokenized_docs,\n",
    "                                        feature_names=feature_names,\n",
    "                                        tfidf=tfidf_matrix)\n",
    "    \n",
    "    evaluation_by_topics[n_topics] = metrics_words\n",
    "\n",
    "evaluation_by_words = {}\n",
    "\n",
    "for n_words in words:\n",
    "    metrics_words = coherence_by_words(df, n = n_words)\n",
    "    evaluation_by_words[n_words] = metrics_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary tables\n",
    "```python\n",
    "def tables(evaluation, type: str):\n",
    "    # Specify the results folder\n",
    "    results = R\"results\"\n",
    "    results_folder = os.path.join(path, results)\n",
    "\n",
    "    # Create individual DataFrames for each specific number\n",
    "    dfs = {}\n",
    "    for n, values in evaluation.items():\n",
    "        dfs[n] = pd.DataFrame(values, columns=['Model', 'Coherence'])\n",
    "\n",
    "    # Save each DataFrame as a PNG file with a title in the specified folder\n",
    "    for n, df in dfs.items():\n",
    "        fig, ax = plt.subplots(figsize=(4, 3))  # Adjust the figure size\n",
    "        ax.axis('off')  # Turn off the axis\n",
    "\n",
    "        # Set the width of the columns\n",
    "        col_width = 1.0 / len(df.columns)\n",
    "        cell_data = [df.columns] + df.values.tolist()  # Include column names as the first row\n",
    "        table = ax.table(cellText=cell_data, loc='center', cellLoc='center', colLabels=None, edges='open')\n",
    "\n",
    "        # Make column labels bold\n",
    "        for (i, j), cell in table.get_celld().items():\n",
    "            if i == 0:\n",
    "                cell.set_text_props(fontweight='bold')\n",
    "\n",
    "        # Adjust column width\n",
    "        table.auto_set_column_width([0, 1])\n",
    "\n",
    "        # Adjust the position of the table within the figure\n",
    "        table.set_fontsize(13)  # Adjust font size\n",
    "        table.scale(1, 2)  # Scale the table\n",
    "\n",
    "        ax.set_title(f'Coherence with {n} {type}', fontsize=18, y=0.95)  # Add a title\n",
    "\n",
    "        filename = os.path.join(results_folder, f'table_{n}_{type}.png')\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0.1)  # Adjust padding\n",
    "        plt.close()  # Close the figure to avoid overlapping when saving multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### By topics\n",
    "```python\n",
    "tables(evaluation_by_topics, 'topics')\n",
    "```\n",
    "\n",
    "![alt text](results/table_5_topics.png)\n",
    "![alt text](results/table_10_topics.png)\n",
    "![alt text](results/table_20_topics.png)\n",
    "![alt text](results/table_50_topics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### By words\n",
    "```python\n",
    "tables(evaluation_by_words, 'words')\n",
    "```\n",
    "\n",
    "![alt text](results/table_10_words.png)\n",
    "![alt text](results/table_100_words.png)\n",
    "![alt text](results/table_1000_words.png)\n",
    "![alt text](results/table_10000_words.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary barplots\n",
    "```python\n",
    "def plots(evaluation, type: str):\n",
    "    for n, metrics in evaluation.items():\n",
    "        model_names, coherence_values = zip(*metrics)\n",
    "\n",
    "        # Create a DataFrame for easy plotting with Seaborn\n",
    "        data = {'Model': model_names, 'Coherence Value': coherence_values}\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        \n",
    "        # Use Seaborn's barplot with the hue parameter\n",
    "        sns.barplot(x='Model', y='Coherence Value', data=df, hue='Model', palette='viridis')\n",
    "        \n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Coherence Value')\n",
    "        plt.title(f'Coherence Evaluation for {n} {type}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        figures_folder = R'~\\path\\figures'\n",
    "        save_path = os.path.join(figures_folder, f'coherence_evaluation_{n}_{type}')\n",
    "        \n",
    "        plt.savefig(save_path)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### By topics\n",
    "```python\n",
    "plots(evaluation_by_topics, 'topics')\n",
    "```\n",
    "![alt text](figures/coherence_evaluation_5_topics.png)\n",
    "![alt text](figures/coherence_evaluation_10_topics.png)\n",
    "![alt text](figures/coherence_evaluation_20_topics.png)\n",
    "![alt text](figures/coherence_evaluation_50_topics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### By words\n",
    "```python\n",
    "plots(evaluation_by_words, 'words')\n",
    "```\n",
    "![alt text](figures/coherence_evaluation_10_words.png)\n",
    "![alt text](figures/coherence_evaluation_100_words.png)\n",
    "![alt text](figures/coherence_evaluation_1000_words.png)\n",
    "![alt text](figures/coherence_evaluation_10000_words.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model results\n",
    "In this section we show the results of the various models with 5 topics and 1000 words.\n",
    "In particular, we print the top 15 words for each topic identified by the model.\n",
    "\n",
    "Another function has been written to show the results based on the model that was stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Display topics\n",
    "```python\n",
    "def display_topics(model_name, model, feature_names):\n",
    "    if model_name == 'LDA' or model_name == 'LSA':\n",
    "        \n",
    "        for topic in model.print_topics(num_words=15):\n",
    "            topic_index, words = topic\n",
    "            topic_words = [word.split(\"*\")[1].strip().strip('\"') for word in words.split(\" + \")]\n",
    "            print(f\"Topic {topic_index + 1}: {', '.join(topic_words)}\")\n",
    "            print('\\n')\n",
    "\n",
    "    elif model_name == 'NMF' or model_name == 'PCA' or model_name == 'RP':\n",
    "        \n",
    "        for topic_index, component in enumerate(model.components_):\n",
    "            component_words = [feature_names[k] for k in component.argsort()[::-1]]\n",
    "            print(f\"Topic {topic_index + 1}: {', '.join(component_words[:15])}\")\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LDA Model\n",
    "```python\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5,\n",
    "                     alpha='symmetric', eta='auto', passes=5, random_state=1)\n",
    "\n",
    "display_topics('LDA', lda_model, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Topic 1: government, gun, state, year, apr, law, car, clinton, tax, drug, netcomcom, ca, case, work, distribution\n",
    "\n",
    "Topic 2: god, israel, christian, israeli, armenian, jew, jesus, believe, turkish, arab, church, bible, kill, religion, apr\n",
    "\n",
    "Topic 3: space, email, post, xnewsreader, computer, pl, bank, tin, science, gordon, distribution, information, version, mail, system\n",
    "\n",
    "Topic 4: window, drive, bike, card, work, henry, problem, system, sale, program, file, run, computer, email, monitor\n",
    "\n",
    "Topic 5: game, team, player, pat, play, win, hockey, season, baseball, year, fan, score, league, wing, playoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LSA model\n",
    "```python\n",
    "lsi_model = LsiModel(corpus, id2word=dictionary, num_topics=5, random_seed = 1)\n",
    "\n",
    "display_topics('LSA', lsi_model, feature_names)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Topic 1: apr, system, work, year, problem, window, distribution, god, computer, drive, run, file, car, state, world\n",
    "\n",
    "Topic 2: window, god, file, card, drive, driver, christian, program, jesus, believe, system, email, disk, problem, video\n",
    "\n",
    "Topic 3: game, team, god, player, win, play, year, season, hockey, christian, jesus, score, baseball, fan, file\n",
    "\n",
    "Topic 4: window, god, key, chip, file, car, clipper, game, encryption, government, christian, team, jesus, win, program\n",
    "\n",
    "Topic 5: drive, car, key, file, window, clipper, chip, encryption, scsi, card, sale, god, program, bike, government"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NMF Model\n",
    "```python\n",
    "nmf_model = NMF(n_components=5, random_state=1).fit(tfidf_matrix)\n",
    "\n",
    "display_topics('NMF', nmf_model, feature_names)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Topic 1: key, apr, government, state, year, car, gun, chip, system, distribution, law, work, clipper, ca, space\n",
    "\n",
    "Topic 2: window, file, program, run, version, help, problem, application, image, graphic, driver, email, display, server, manager\n",
    "\n",
    "Topic 3: game, team, win, player, play, year, season, hockey, score, baseball, fan, nhl, playoff, league, run\n",
    "\n",
    "Topic 4: god, christian, jesus, believe, bible, christ, faith, church, atheist, life, religion, belief, truth, christianity, sin\n",
    "\n",
    "Topic 5: drive, card, sale, scsi, mb, disk, monitor, mac, driver, video, system, problem, hard, controller, work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA Model\n",
    "```python\n",
    "tfidf_matrix_dense = tfidf_matrix.todense() if sparse.issparse(tfidf_matrix) else tfidf_matrix\n",
    "\n",
    "# Convert to numpy array\n",
    "tfidf_matrix_array = np.asarray(tfidf_matrix_dense)\n",
    "\n",
    "# Centering\n",
    "mean_tfidf = np.mean(tfidf_matrix_array, axis=0)  # Calculate the mean of each column\n",
    "centered_tfidf_matrix = tfidf_matrix_array - mean_tfidf\n",
    "\n",
    "pca_model = PCA(n_components=5, random_state=1).fit(centered_tfidf_matrix)\n",
    "\n",
    "display_topics('PCA', pca_model, feature_names)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Topic 1: god, christian, jesus, believe, year, law, bible, team, life, jew, church, religion, christ, faith, game\n",
    "\n",
    "Topic 2: game, team, player, win, play, year, season, hockey, score, baseball, fan, nhl, playoff, league, toronto\n",
    "\n",
    "Topic 3: key, chip, car, clipper, encryption, government, gun, system, phone, drive, algorithm, buy, netcomcom, public, law\n",
    "\n",
    "Topic 4: drive, car, scsi, sale, card, god, bike, mb, price, buy, hard, ide, speed, controller, disk\n",
    "\n",
    "Topic 5: god, key, chip, card, game, drive, scsi, clipper, encryption, system, team, mb, jesus, christian, disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RP Model\n",
    "```python\n",
    "rp_model = GaussianRandomProjection(n_components=5, random_state=1).fit(tfidf_matrix)\n",
    "\n",
    "display_topics('RP', rp_model, feature_names)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Topic 1: modem, recent, explain, communication, power, decide, step, side, spend, comment, claim, attack, georgia, hardware, application\n",
    "\n",
    "Topic 2: output, definition, fit, hand, american, however, political, average, certainly, create, replace, wm, price, illinois, select\n",
    "\n",
    "Topic 3: handle, sit, single, describe, red, design, center, development, range, score, charge, later, bike, report, email\n",
    "\n",
    "Topic 4: apply, tim, software, tin, suspect, wm, express, inside, uunet, feel, chicago, suppose, aid, memory, video\n",
    "\n",
    "Topic 5: build, hell, defense, muslim, month, radio, men, several, depend, company, data, half, dave, flame, yous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
