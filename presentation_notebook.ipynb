{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project overview\n",
    "\n",
    "The goal of the project is to evaluate how different models perform in the task of topic modelling.\n",
    "The models used by the paper are:\n",
    "1. Latent Dirchlet Allocation (LDA)\n",
    "2. Latent Semantic Analysis (LSA)\n",
    "3. Non-Negative Matrix Factorization (NMF)\n",
    "4. Principal Component Analysis (PCA)\n",
    "5. Random Projection (RP)\n",
    "\n",
    "The evaluation performed by the authors consisted in examining models performances when changing the **number of topics** or the **number of words**, keeping all the others parameters fixed.\n",
    "\n",
    "The authors didn't specify which parameters were fixed while performing evaluation, therefore we had to choose our own settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importing and preprocessing\n",
    "The data used comes from the 20-Newsgroups dataset, available online, containing various texts from different topics. The importing required preprocessing steps such as:\n",
    "- removal of special characters using regular expressions\n",
    "- tokenization\n",
    "- lemmatization with the aid of Part Of Speech\n",
    "\n",
    "Below the code used for importing and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import os\n",
    "import re, string\n",
    "from nltk.corpus import words, wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def import_data(path):\n",
    "    \n",
    "    folders_path_list = []\n",
    "    folders = os.listdir(path)\n",
    "    \n",
    "    for folder in folders:\n",
    "        folders_path_list.append(os.path.join(path, folder))\n",
    "    \n",
    "    conversion = {'religion' : [folders[i] for i in [0, 15, 19]],\n",
    "                  'computer': [folders[i] for i in range(1, 6)],\n",
    "                  'science': [folders[i] for i in range(11, 15)],\n",
    "                  'politics': [folders[i] for i in range(16, 19)],\n",
    "                  'misc': [folders[6]],\n",
    "                  'recreation': [folders[i] for i in range(7, 11)]}\n",
    "    \n",
    "    topics = folders.copy()\n",
    "    \n",
    "    for j, folder in enumerate(folders):\n",
    "        for topic, values in conversion.items():\n",
    "            \n",
    "            if folder in values:\n",
    "                topics[j] = topic\n",
    "    \n",
    "    \n",
    "    column_names = ['File_Name', 'Content', 'Folder', 'Topic']\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    for folder, folder_path, topic in zip(folders, folders_path_list, topics):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                content = file.read()\n",
    "                df = pd.concat([df,\n",
    "                                pd.DataFrame({'File_Name': [file_name], 'Content': [content],\n",
    "                                              'Folder' : [folder], 'Topic': [topic]})],\n",
    "                               ignore_index=True)\n",
    "                \n",
    "    df['Content'] = df['Content'].astype(\"string\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def preprocess(doc):\n",
    "    \n",
    "    # Remove email addresses\n",
    "    doc = re.sub(r'\\b\\S*@\\S*\\.\\S*\\b', '', doc)\n",
    "    \n",
    "    # Remove special characters and digits, retain only words with letters\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "    \n",
    "    # Lowercase and strip\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # Remove brackets of any kind\n",
    "    doc = re.sub(r'[(){}[\\]]', '', doc)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    doc = doc.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Tokenize document\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    # POS tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "     # map POS tags to WordNet POS tags\n",
    "    tag_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # .get() returns value associated to keyname. If keyname is not a key, it returns what's specified in value\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, tag_map.get(pos[0], wordnet.NOUN)) for token, pos in pos_tags]\n",
    "    \n",
    "    \n",
    "    # Filter stopwords out of lemmatized tokens\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    stop_words.extend(['hi', 'thanks', 'lot', 'dont', 'article', 'everyone', 'anyone',\n",
    "                       'someone', 'nothing',\n",
    "                       'something', 'anything', 'everybody', 'somebody', 'anybody',\n",
    "                       'please', 'ask', 'people', 'university',\n",
    "                       'question', 'yeah', 'shouldnt', 'theyre', 'thing', 'theyll', 'didnt', 'sorry', 'hey',\n",
    "                       'oh', 'thats', 'thank', 'cannot', 'right', 'would', 'one', 'get', 'know', 'like', 'use', 'go',\n",
    "                       'think', 'make', 'say', 'see', 'also', 'could', 'well', 'want', 'way', 'take', 'find', 'need', 'try',\n",
    "                       'much', 'come', 'many', 'may', 'give', 'really', 'tell', 'two', 'still', 'read', 'might', 'write',\n",
    "                       'never', 'look', 'sure', 'day', 'even', 'new', 'time', 'good', 'first', 'keep', 'since', 'last', \n",
    "                       'long', 'fact', 'must', 'cant', 'another', 'little', 'without', 'csutexasedu', 'nntppostinghost',\n",
    "                       'im', 'seem', 'replyto', 'let', 'group', 'call', 'seem', ])\n",
    "    \n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    \n",
    "    # Recreate the document\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "path = R\"~\\path\"\n",
    "\n",
    "df = import_data(path)\n",
    "\n",
    "df['Clean_Content'] = df['Content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TFIDF and other objects\n",
    "Once the dataset has been imported and preprocessed, a TFIDF matrix was constructed as input matrix for the models. To do this, it was convenient to write another function that could be called inside the notebook. Besides the tfidf matrix, given a specific set of parameters, the function also returned some objects required for topic modelling with the gensim package, which is the one used for some of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, matutils\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_tfidf_tokendocs_corpus_dict(df, max_df, min_df, max_features): #0.5, 5, 5000\n",
    "    # convert text into lists\n",
    "    documents = df['Clean_Content'].tolist()\n",
    "    documents\n",
    "\n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, min_df = min_df, norm = 'l2', max_features=max_features)\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # this is a list of documents with tokens\n",
    "    # it's needed for the coherence function\n",
    "    tokenized_docs = [word_tokenize(document) for document in documents]\n",
    "    \n",
    "    # Convert TF-IDF matrix to Gensim corpus\n",
    "    corpus = matutils.Sparse2Corpus(tfidf_matrix.transpose())\n",
    "    # Convert the document-term matrix to a gensim Dictionary\n",
    "    dictionary = corpora.Dictionary.from_corpus(corpus,\n",
    "                                            id2word=dict((id, word) for word, id in vectorizer.vocabulary_.items()))\n",
    "    \n",
    "    return tfidf_matrix, feature_names, tokenized_docs, corpus, dictionary\n",
    "\n",
    "\n",
    "tfidf_matrix, feature_names, tokenized_docs, corpus, dictionary = get_tfidf_tokendocs_corpus_dict(df, max_df=0.5, min_df=5, max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models evaluation\n",
    "The evaluation of the models was done by comparing Coherence, a measure of topics quality. We replicated the analysis of the paper by comparing the goodness of the models for different number of topics (5, 10, 20, 50) and different number of words (10, 100, 1000, 10000).\n",
    "\n",
    "To perform the evaluation, we chose to write functions that ran the models different times for the specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, matutils\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import LdaModel, LsiModel, CoherenceModel\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "topics = [5, 10, 20, 50]\n",
    "words = [10, 100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation by topics:\n",
    "```python\n",
    "def coherence_by_topics(n: int, corpus, dictionary, texts, feature_names, tfidf):\n",
    "    models = ['LDA', 'LSA', 'NMF', 'PCA', 'RP']\n",
    "\n",
    "    coherence = []\n",
    "\n",
    "    for model_name in models:\n",
    "        if model_name == 'LDA' or model_name == 'LSA':\n",
    "            \n",
    "            if model_name == 'LDA':\n",
    "                model = LdaModel(corpus=corpus, id2word=dictionary,\n",
    "                                num_topics = n,\n",
    "                                alpha='symmetric', eta='auto', passes=5,\n",
    "                                random_state=1)\n",
    "        \n",
    "            elif model_name == 'LSA':\n",
    "                model = LsiModel(corpus, id2word=dictionary,\n",
    "                                num_topics=n, random_seed=1)\n",
    "            \n",
    "            coherence_value = CoherenceModel(model=model,\n",
    "                                            dictionary = dictionary,\n",
    "                                            texts=texts,\n",
    "                                            coherence='c_v').get_coherence()\n",
    "            coherence.append(coherence_value)\n",
    "\n",
    "        elif model_name == 'NMF' or model_name == 'PCA' or model_name == 'RP':\n",
    "            \n",
    "            if model_name == 'NMF':\n",
    "                \n",
    "                model = NMF(n_components=n,\n",
    "                            random_state=1, max_iter=600).fit(tfidf)\n",
    "                \n",
    "            elif model_name == 'RP':\n",
    "                \n",
    "                model = GaussianRandomProjection(n_components=n,\n",
    "                                                random_state=1).fit(tfidf)\n",
    "                \n",
    "            elif model_name == 'PCA':\n",
    "                # Convert sparse matrix to dense. PCA cannot be done on sparse matrixes\n",
    "                tfidf_matrix_dense = tfidf.todense()\n",
    "                            if sparse.issparse(tfidf) else tfidf\n",
    "\n",
    "                # Convert to numpy array\n",
    "                tfidf_matrix_array = np.asarray(tfidf_matrix_dense)\n",
    "\n",
    "                # Centering\n",
    "                mean_tfidf = np.mean(tfidf_matrix_array,\n",
    "                                    axis=0)\n",
    "                centered_tfidf_matrix = tfidf_matrix_array - mean_tfidf\n",
    "                \n",
    "                model = PCA(n_components=n,\n",
    "                            random_state=1).fit(centered_tfidf_matrix)\n",
    "                \n",
    "            # Retrieve top words for each component\n",
    "            topics = []\n",
    "            for j, component in enumerate(model.components_):\n",
    "                component_words = [(feature_names[k], component[k])\n",
    "                                    for k in component.argsort()[::-1]]\n",
    "                topics.append(component_words)\n",
    "                \n",
    "            topics_for_coherence = [[word for word, _ in topic]\n",
    "                                        for topic in topics]\n",
    "            \n",
    "            coherence_value = CoherenceModel(topics=topics_for_coherence,\n",
    "                                            texts=texts, dictionary=dictionary,\n",
    "                                            coherence='c_v').get_coherence()\n",
    "            coherence_value = round(coherence_value, 4)\n",
    "            \n",
    "            coherence.append(coherence_value)\n",
    "    \n",
    "    coherence = [round(num, 4) for num in coherence]\n",
    "\n",
    "    return list(zip(models, coherence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation by words:\n",
    "```python\n",
    "def coherence_by_words(df, n):\n",
    "    models = ['LDA', 'LSA', 'NMF', 'PCA', 'RP']\n",
    "\n",
    "    documents = df['Clean_Content'].tolist()\n",
    "    documents\n",
    "\n",
    "    coherence = []\n",
    "\n",
    "    for model_name in models:\n",
    "        # TF-IDF Vectorization\n",
    "        vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 5,\n",
    "                                    norm = 'l2', max_features=n)\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # this is a list of documents with tokens\n",
    "        # it's needed for the coherence function\n",
    "        texts = [word_tokenize(document) for document in documents]\n",
    "        \n",
    "        # Convert TF-IDF matrix to Gensim corpus\n",
    "        corpus = matutils.Sparse2Corpus(tfidf_matrix.transpose())\n",
    "        # Convert the document-term matrix to a gensim Dictionary\n",
    "        dictionary = corpora.Dictionary.from_corpus(corpus,\n",
    "                                                id2word=dict((id, word)\n",
    "                                                for word, id in\n",
    "                                                vectorizer.vocabulary_.items()))\n",
    "        \n",
    "        if model_name == 'LDA' or model_name == 'LSA':\n",
    "\n",
    "            if model_name == 'LDA':\n",
    "                model = LdaModel(corpus=corpus,\n",
    "                                id2word=dictionary,\n",
    "                                num_topics = 5,\n",
    "                                alpha='symmetric',\n",
    "                                eta='auto', passes=5,\n",
    "                                random_state=1)\n",
    "        \n",
    "            elif model_name == 'LSA':\n",
    "                model = LsiModel(corpus,\n",
    "                                id2word=dictionary,\n",
    "                                num_topics=5,\n",
    "                                random_seed=1)\n",
    "            \n",
    "            coherence_value = CoherenceModel(model=model,\n",
    "                                            dictionary = dictionary,\n",
    "                                            texts=texts,\n",
    "                                            coherence='c_v').get_coherence()\n",
    "            coherence.append(coherence_value)\n",
    "\n",
    "        elif model_name == 'NMF' or model_name == 'PCA' or model_name == 'RP':\n",
    "            \n",
    "            if model_name == 'NMF':\n",
    "                \n",
    "                model = NMF(n_components=5,\n",
    "                            random_state=1,\n",
    "                            max_iter=600).fit(tfidf_matrix)\n",
    "                \n",
    "            elif model_name == 'RP':\n",
    "                \n",
    "                model = GaussianRandomProjection(n_components=5,\n",
    "                                                random_state=1).fit(tfidf_matrix)\n",
    "                \n",
    "            elif model_name == 'PCA':\n",
    "                # Convert sparse matrix to dense. PCA cannot be done on sparse matrixes\n",
    "                tfidf_matrix_dense = tfidf_matrix.todense()\n",
    "                                        if sparse.issparse(tfidf_matrix)\n",
    "                                        else tfidf_matrix\n",
    "\n",
    "                # Convert to numpy array\n",
    "                tfidf_matrix_array = np.asarray(tfidf_matrix_dense)\n",
    "\n",
    "                # Centering with respect to the columns\n",
    "                mean_tfidf = np.mean(tfidf_matrix_array,\n",
    "                                    axis=0)\n",
    "                centered_tfidf_matrix = tfidf_matrix_array - mean_tfidf\n",
    "                \n",
    "                model = PCA(n_components=5,\n",
    "                            random_state=1).fit(centered_tfidf_matrix)\n",
    "                \n",
    "            # Retrieve top words for each component\n",
    "            components = []\n",
    "            for j, component in enumerate(model.components_):\n",
    "                component_words = [(feature_names[k], component[k])\n",
    "                                    for k in component.argsort()[::-1]]\n",
    "                components.append(component_words)\n",
    "                \n",
    "            topics_for_coherence = [[word for word, _ in component]\n",
    "                                        for component in components]\n",
    "            \n",
    "            coherence_value = CoherenceModel(topics=topics_for_coherence,\n",
    "                                            texts=texts,\n",
    "                                            dictionary=dictionary,\n",
    "                                            coherence='c_v').get_coherence()\n",
    "            coherence_value = round(coherence_value, 4)\n",
    "            \n",
    "            coherence.append(coherence_value)\n",
    "    \n",
    "    coherence = [round(num, 4) for num in coherence]\n",
    "        \n",
    "    return list(zip(models, coherence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Storing results\n",
    "```python\n",
    "from coherence_by_topics import coherence_by_topics\n",
    "from coherence_by_words import coherence_by_words\n",
    "\n",
    "evaluation_by_topics = {}\n",
    "\n",
    "for n_topics in topics:\n",
    "    metrics_words = coherence_by_topics(n = n_topics,\n",
    "                                        corpus=corpus,\n",
    "                                        dictionary=dictionary,\n",
    "                                        texts=tokenized_docs,\n",
    "                                        feature_names=feature_names,\n",
    "                                        tfidf=tfidf_matrix)\n",
    "    \n",
    "    evaluation_by_topics[n_topics] = metrics_words\n",
    "\n",
    "evaluation_by_words = {}\n",
    "\n",
    "for n_words in words:\n",
    "    metrics_words = coherence_by_words(df, n = n_words)\n",
    "    evaluation_by_words[n_words] = metrics_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Results for evaluation by topics:\n",
    "```python\n",
    "evaluation_by_topics[5]\n",
    "'''\n",
    "[('LDA', 0.5179),\n",
    " ('LSA', 0.5469),\n",
    " ('NMF', 0.6479),\n",
    " ('PCA', 0.5464),\n",
    " ('RP', 0.2019)]'''\n",
    "\n",
    "evaluation_by_topics[10]\n",
    "'''\n",
    "[('LDA', 0.5264),\n",
    " ('LSA', 0.4587),\n",
    " ('NMF', 0.6736),\n",
    " ('PCA', 0.4517),\n",
    " ('RP', 0.2012)]'''\n",
    "\n",
    "evaluation_by_topics[20]\n",
    "'''\n",
    "[('LDA', 0.3957),\n",
    " ('LSA', 0.382),\n",
    " ('NMF', 0.633),\n",
    " ('PCA', 0.3656),\n",
    " ('RP', 0.2071)]'''\n",
    "\n",
    " evaluation_by_topics[50]\n",
    " '''\n",
    " [('LDA', 0.3593),\n",
    " ('LSA', 0.3138),\n",
    " ('NMF', 0.5271),\n",
    " ('PCA', 0.3193),\n",
    " ('RP', 0.2112)]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Results for evaluation by words:\n",
    "```python\n",
    "evaluation_by_words[10]\n",
    "'''\n",
    "[('LDA', 0.4661),\n",
    " ('LSA', 0.4661),\n",
    " ('NMF', 0.4661),\n",
    " ('PCA', 0.4661),\n",
    " ('RP', 0.4661)]'''\n",
    "\n",
    " evaluation_by_words[100]\n",
    "'''\n",
    "[('LDA', 0.4183),\n",
    " ('LSA', 0.3723),\n",
    " ('NMF', 0.4723),\n",
    " ('PCA', 0.387),\n",
    " ('RP', 0.3258)]'''\n",
    "\n",
    " evaluation_by_words[1000]\n",
    "'''\n",
    "[('LDA', 0.5179),\n",
    " ('LSA', 0.5469),\n",
    " ('NMF', 0.6479),\n",
    " ('PCA', 0.5464),\n",
    " ('RP', 0.2019)]'''\n",
    "\n",
    " evaluation_by_words[10000]\n",
    "'''\n",
    "[('LDA', 0.4755),\n",
    " ('LSA', 0.4596),\n",
    " ('NMF', 0.7112),\n",
    " ('PCA', 0.625),\n",
    " ('RP', 0.6706)]'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
