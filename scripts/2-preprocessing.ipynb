{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# run this cell if this package is not installed\n",
    "# %pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run 1-setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_set = set(words.words())\n",
    "\n",
    "def is_english_word(word):\n",
    "    return word.lower() in english_word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # we extend the stop words to avoid retaining based on the words that appear in the final model\n",
    "    stop_words.extend(['hi', 'thanks', 'lot', 'dont', 'article', 'everyone', 'anyone', 'someone', 'nothing',\n",
    "                       'something', 'anything', 'everybody', 'somebody', 'anybody', 'please', 'ask', 'mo', 'mon', 'eh', 'da', \n",
    "                       'th', 'dude', 'sh', 'ra', 'li', 'ce', 'people', 'university', 'dod',\n",
    "                       'question', 'yeah', 'shouldnt', 'theyre', 'thing', 'theyll', 'didnt', 'sorry', 'hey',\n",
    "                       'oh', 'thats', 'thank', 'cannot', 'right'])\n",
    "    \n",
    "    # remove the names and surnames that appear in the final models\n",
    "    stop_words.extend(['kent', 'sandvik', 'spencer', 'peter', 'henry', 'jack', 'frank', 'smith',\n",
    "                       'larry', 'marc', 'miller', 'mike', 'jeff', 'martin', 'bob', 'alan', 'rob', ])\n",
    "    \n",
    "    # remove email addresses\n",
    "    doc = re.sub(r'\\b\\S*@\\S*\\.\\S*\\b', '', doc)\n",
    "    \n",
    "    # remove special characters and digits, retaining only words with letters\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "    \n",
    "    # lowercase and strip\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    # remove brackets of any kind\n",
    "    doc = re.sub(r'[(){}[\\]]', '', doc)\n",
    "    \n",
    "    # remove punctuation\n",
    "    doc = doc.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # retain only English words\n",
    "    doc = ' '.join(word for word in doc.split() if is_english_word(word))\n",
    "    \n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    \n",
    "    # determine POS of the tokens\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # # map POS tags to WordNet POS tags\n",
    "    # tag_map = {\n",
    "    #     'N': wordnet.NOUN,\n",
    "    #     'V': wordnet.VERB,\n",
    "    #     'R': wordnet.ADV,\n",
    "    #     'J': wordnet.ADJ\n",
    "    # }\n",
    "\n",
    "    # # lemmatize the tokens\n",
    "    # lemmatized_tokens = [lemmatizer.lemmatize(token, tag_map.get(pos[0], wordnet.NOUN)) for token, pos in pos_tags]\n",
    "    \n",
    "    # keep only nouns\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, wordnet.NOUN) for token, pos in pos_tags if pos.startswith('N')]\n",
    "    \n",
    "    # filter stopwords out of lemmatized tokens \n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    \n",
    "    # recreate the document\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df['Content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'surface organization research center canada surface topic information surface help surface plane cut cylinder cone plane course surface patch vanishing curvature ie matrix patch surface plane sphere earth way curvature look book geometry book author geometry year publisher note enjoy st canada'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No empty documents found.\n"
     ]
    }
   ],
   "source": [
    "# Check for empty documents\n",
    "empty_documents = df_clean[df_clean.str.strip() == '']\n",
    "\n",
    "# Count the number of empty documents\n",
    "num_empty_documents = len(empty_documents)\n",
    "\n",
    "if num_empty_documents > 0:\n",
    "    print(f\"Number of empty documents: {num_empty_documents}\")\n",
    "    print(\"Indices of empty documents:\")\n",
    "    print(empty_documents.index)\n",
    "else:\n",
    "    print(\"No empty documents found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
