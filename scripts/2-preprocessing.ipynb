{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# run this cell if this package is not installed\n",
    "# %pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run 1-setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_set = set(words.words())\n",
    "\n",
    "def is_english_word(word):\n",
    "    return word.lower() in english_word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # remove email addresses\n",
    "    doc = re.sub(r'\\b\\S*@\\S*\\.\\S*\\b', '', doc)\n",
    "    \n",
    "    # remove special characters and digits, retaining only words with letters\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "    \n",
    "    # lowercase and strip\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    # remove brackets of any kind\n",
    "    doc = re.sub(r'[(){}[\\]]', '', doc)\n",
    "    \n",
    "    # remove punctuation\n",
    "    doc = doc.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # retain only English words\n",
    "    doc = ' '.join(word for word in doc.split() if is_english_word(word))\n",
    "    \n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    \n",
    "    # determine POS of the tokens\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # # map POS tags to WordNet POS tags\n",
    "    # tag_map = {\n",
    "    #     'N': wordnet.NOUN,\n",
    "    #     'V': wordnet.VERB,\n",
    "    #     'R': wordnet.ADV,\n",
    "    #     'J': wordnet.ADJ\n",
    "    # }\n",
    "\n",
    "    # # lemmatize the tokens\n",
    "    # lemmatized_tokens = [lemmatizer.lemmatize(token, tag_map.get(pos[0], wordnet.NOUN)) for token, pos in pos_tags]\n",
    "    \n",
    "    # keep only nouns\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, wordnet.NOUN) for token, pos in pos_tags if pos.startswith('N')]\n",
    "    \n",
    "    # filter stopwords out of lemmatized tokens \n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    \n",
    "    # recreate the document\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df['Content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'surface organization research center canada article hi surface anyone topic information surface thanks help surface plane cut cylinder cone plane course surface patch vanishing curvature ie matrix patch surface plane sphere earth way curvature look book geometry book author geometry year publisher note enjoy university university st canada'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.iloc[1000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
