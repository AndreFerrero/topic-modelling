{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run full_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfidf_corpus_dictionary import get_tfidf_tokendocs_corpus_dict\n",
    "from gensim.models import LdaModel, LsiModel, CoherenceModel\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function we get various objects needed for modelling:\n",
    "1. TFIDF matrix as input data, with specified parameters\n",
    "2. feature names as the words retained with TFIDF\n",
    "3. tokenized documents, a list of lists, where the inner lists contain the tokens for each document\n",
    "4. corpus, gensim object needed for modelling with that package\n",
    "4. dictionary, gensim object containing informations on the words of the corpus and their positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix, feature_names, tokenized_docs, corpus, dictionary = get_tfidf_tokendocs_corpus_dict(df, max_df=0.5, min_df=5, max_features=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll evaluate different topic models based on Coherence score.\n",
    "\n",
    "Coherence is a metric used to evalute topics quality. The higher the coherence, the better the model did in creating the topics.\n",
    "\n",
    "For every model we'll use a function to retrieve coherence for different numbers of topics (5, 10, 15, 20, 50). This information will be used to evaluate how the models performed as the number of topics changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coherence_topics import coherence_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = dict()\n",
    "models = ['LDA', 'LSA', 'NMF', 'PCA', 'RP']\n",
    "\n",
    "for mod in models:\n",
    "    metrics = coherence_topics(model_name=mod, corpus=corpus, dictionary=dictionary,\n",
    "                               texts=tokenized_docs, feature_names=feature_names, tfidf=tfidf_matrix)\n",
    "    evaluation[mod] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.545979304624336),\n",
       " (10, 0.5035821857797423),\n",
       " (15, 0.5275933069581035),\n",
       " (20, 0.5134451552900542),\n",
       " (50, 0.46410568089530935)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation['LDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.38657061293889755),\n",
       " (10, 0.46671223720227306),\n",
       " (15, 0.4080013163682798),\n",
       " (20, 0.37904110899562504),\n",
       " (50, 0.3633260790835298)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation['LSA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.7147033104403053),\n",
       " (10, 0.7358479263967541),\n",
       " (15, 0.7352788685093538),\n",
       " (20, 0.7255836088832905),\n",
       " (50, 0.6186516384203021)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation['NMF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.6080252573570404),\n",
       " (10, 0.5470605282364212),\n",
       " (15, 0.4786088251630099),\n",
       " (20, 0.43360202923035673),\n",
       " (50, 0.35129601997751037)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation['PCA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.5388460051693942),\n",
       " (10, 0.5435245527262123),\n",
       " (15, 0.5373158420089561),\n",
       " (20, 0.5206040179267396),\n",
       " (50, 0.5219495119538514)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation['RP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit the LDA model with the number of topics that yields the highest coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5,\n",
    "                     alpha='symmetric', eta='auto', passes=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in lda_model.print_topics(num_words=15):\n",
    "    topic_index, words = topic\n",
    "    word_list = [word.split(\"*\")[1].strip().strip('\"') for word in words.split(\" + \")]\n",
    "    print(f\"Topic {topic_index}: {', '.join(word_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis, pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the LDA model using pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word, mds='tsne')\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to do the same for LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = LsiModel(corpus, id2word=dictionary, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in lsi_model.print_topics():\n",
    "    topic_index, words = topic\n",
    "    word_list = [word.split(\"*\")[1].strip().strip('\"') for word in words.split(\" + \")]\n",
    "    print(f\"Topic {topic_index}: {', '.join(word_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
