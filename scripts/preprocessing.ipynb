{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# run this cell if this package is not installed\n",
    "# !pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_set = set(words.words())\n",
    "\n",
    "def is_english_word(word):\n",
    "    return word.lower() in english_word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # sentences to be removed\n",
    "    p1 = \"Chat Conversation Start\"\n",
    "    p2 = \"Chat Conversation End\"\n",
    "    \n",
    "    # Remove p1 only the first time it appears\n",
    "    doc = re.sub(rf'^\\s*{re.escape(p1)}\\s*', '', doc)\n",
    "\n",
    "    # Remove p2 only the last time it appears\n",
    "    doc = re.sub(rf'\\s*{re.escape(p2)}\\s*$', '', doc)\n",
    "    \n",
    "    # remove email addresses\n",
    "    doc = re.sub(r'\\b\\S*@\\S*\\.\\S*\\b', '', doc)\n",
    "    \n",
    "    # remove special characters and digits, retaining only words with letters\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "    \n",
    "    # lowercase and strip\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    # remove brackets of any kind\n",
    "    doc = re.sub(r'[(){}[\\]]', '', doc)\n",
    "    \n",
    "    # remove punctuation\n",
    "    doc = doc.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # retain only English words\n",
    "    doc = ' '.join(word for word in doc.split() if is_english_word(word))\n",
    "    \n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    \n",
    "    # determine POS of the tokens\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # map POS tags to WordNet POS tags\n",
    "    tag_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "\n",
    "    # lemmatize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, tag_map.get(pos[0], wordnet.NOUN)) for token, pos in pos_tags]\n",
    "    \n",
    "    # filter stopwords out of lemmatized tokens \n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    \n",
    "    # recreate the document\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df['Content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject developable surface organization university hong tin version hi currently developable surface anyone familiar topic give information allow find developable surface thanks help'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>abacus</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abate</th>\n",
       "      <th>abatement</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbot</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>...</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooid</th>\n",
       "      <th>zoological</th>\n",
       "      <th>zoology</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zucchini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11331</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11332</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11333</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11334</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11335</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11336 rows Ã— 20362 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  abacus  abandon  abandoned  abandonment  abate  abatement  abbey  \\\n",
       "0      0.0     0.0      0.0        0.0          0.0    0.0        0.0    0.0   \n",
       "1      0.0     0.0      0.0        0.0          0.0    0.0        0.0    0.0   \n",
       "2      0.0     0.0      0.0        0.0          0.0    0.0        0.0    0.0   \n",
       "3      0.0     0.0      0.0        0.0          0.0    0.0        0.0    0.0   \n",
       "4      0.0     0.0      0.0        0.0          0.0    0.0        0.0    0.0   \n",
       "...    ...     ...      ...        ...          ...    ...        ...    ...   \n",
       "11331  0.0     0.0      0.0        0.0          0.0    0.0        0.0    0.0   \n",
       "11332  0.0     0.0      0.0        0.0          0.0    0.0        0.0    0.0   \n",
       "11333  0.0     0.0      0.0        0.0          0.0    0.0        0.0    0.0   \n",
       "11334  0.0     0.0      0.0        0.0          0.0    0.0        0.0    0.0   \n",
       "11335  0.0     0.0      0.0        0.0          0.0    0.0        0.0    0.0   \n",
       "\n",
       "       abbot  abbreviation  ...  zombie  zone  zoning  zoo  zooid  zoological  \\\n",
       "0        0.0           0.0  ...     0.0   0.0     0.0  0.0    0.0         0.0   \n",
       "1        0.0           0.0  ...     0.0   0.0     0.0  0.0    0.0         0.0   \n",
       "2        0.0           0.0  ...     0.0   0.0     0.0  0.0    0.0         0.0   \n",
       "3        0.0           0.0  ...     0.0   0.0     0.0  0.0    0.0         0.0   \n",
       "4        0.0           0.0  ...     0.0   0.0     0.0  0.0    0.0         0.0   \n",
       "...      ...           ...  ...     ...   ...     ...  ...    ...         ...   \n",
       "11331    0.0           0.0  ...     0.0   0.0     0.0  0.0    0.0         0.0   \n",
       "11332    0.0           0.0  ...     0.0   0.0     0.0  0.0    0.0         0.0   \n",
       "11333    0.0           0.0  ...     0.0   0.0     0.0  0.0    0.0         0.0   \n",
       "11334    0.0           0.0  ...     0.0   0.0     0.0  0.0    0.0         0.0   \n",
       "11335    0.0           0.0  ...     0.0   0.0     0.0  0.0    0.0         0.0   \n",
       "\n",
       "       zoology  zoom  zorro  zucchini  \n",
       "0          0.0   0.0    0.0       0.0  \n",
       "1          0.0   0.0    0.0       0.0  \n",
       "2          0.0   0.0    0.0       0.0  \n",
       "3          0.0   0.0    0.0       0.0  \n",
       "4          0.0   0.0    0.0       0.0  \n",
       "...        ...   ...    ...       ...  \n",
       "11331      0.0   0.0    0.0       0.0  \n",
       "11332      0.0   0.0    0.0       0.0  \n",
       "11333      0.0   0.0    0.0       0.0  \n",
       "11334      0.0   0.0    0.0       0.0  \n",
       "11335      0.0   0.0    0.0       0.0  \n",
       "\n",
       "[11336 rows x 20362 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv = TfidfVectorizer(min_df=0., max_df=1., norm=\"l2\",\n",
    "                     use_idf=True, smooth_idf=True)\n",
    "tv_matrix = tv.fit_transform(df_clean)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "vocab = tv.get_feature_names_out()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ask        0.08\n",
       "       back       0.06\n",
       "       bed        0.14\n",
       "       cat        0.12\n",
       "       city       0.09\n",
       "                  ... \n",
       "11335  without    0.06\n",
       "       work       0.02\n",
       "       would      0.02\n",
       "       write      0.03\n",
       "       yep        0.06\n",
       "Length: 756273, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which entries are non zero?\n",
    "tfidf = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
    "tfdidf_nonzero = tfidf[tfidf != 0].stack()\n",
    "tfdidf_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ask           0.08\n",
       "back          0.06\n",
       "bed           0.14\n",
       "cat           0.12\n",
       "city          0.09\n",
       "come          0.06\n",
       "crazy         0.23\n",
       "enjoy         0.11\n",
       "everything    0.16\n",
       "excite        0.14\n",
       "fantastic     0.14\n",
       "fine          0.09\n",
       "friend        0.36\n",
       "fun           0.10\n",
       "go            0.05\n",
       "good          0.29\n",
       "great         0.14\n",
       "happy         0.10\n",
       "home          0.08\n",
       "hope          0.16\n",
       "house         0.09\n",
       "ill           0.08\n",
       "jersey        0.12\n",
       "last          0.06\n",
       "later         0.09\n",
       "like          0.04\n",
       "long          0.13\n",
       "lot           0.20\n",
       "lovely        0.15\n",
       "maybe         0.07\n",
       "meet          0.10\n",
       "miss          0.19\n",
       "new           0.11\n",
       "news          0.08\n",
       "one           0.04\n",
       "plan          0.10\n",
       "really        0.12\n",
       "remember      0.08\n",
       "repeat        0.11\n",
       "sad           0.12\n",
       "safe          0.10\n",
       "see           0.05\n",
       "since         0.06\n",
       "smile         0.14\n",
       "soon          0.18\n",
       "spend         0.09\n",
       "spent         0.14\n",
       "still         0.06\n",
       "time          0.24\n",
       "travel        0.23\n",
       "try           0.06\n",
       "want          0.06\n",
       "week          0.09\n",
       "well          0.05\n",
       "work          0.05\n",
       "worry         0.10\n",
       "yah           0.16\n",
       "yes           0.07\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdidf_nonzero[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
