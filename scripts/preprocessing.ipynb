{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# run this cell if this package is not installed\n",
    "# !pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove p1 and p2\n",
    "def remove_start_end(text):\n",
    "    \n",
    "    p1 = \"Chat Conversation Start\"\n",
    "    p2 = \"Chat Conversation End\"\n",
    "    \n",
    "    # Remove p1 only the first time it appears\n",
    "    text = re.sub(rf'^\\s*{re.escape(p1)}\\s*', '', text)\n",
    "\n",
    "    # Remove p2 only the last time it appears\n",
    "    text = re.sub(rf'\\s*{re.escape(p2)}\\s*$', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the function to the \"Content\" column\n",
    "df['Content'] = df['Content'].apply(remove_start_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # remove email addresses\n",
    "    doc = re.sub(r'\\b\\S*@\\S*\\.\\S*\\b', '', doc)\n",
    "    # lowercase and remove special characters\\whitespace\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', str(doc), re.I | re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "            \n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        u good girls bed haha omg miss lot lovely frie...\n",
       "1        u hi u great thanks kind fine think problem as...\n",
       "2        u hello u hello hello best friend finally real...\n",
       "3        u want ask study lot mathematics business engi...\n",
       "4        u hi checked courses ask friends see one took ...\n",
       "                               ...                        \n",
       "11331    robert weiss subject apr gods promise philippi...\n",
       "11332    kent sandvik subject apr gods promise john org...\n",
       "11333    kent sandvik subject disillusioned protestant ...\n",
       "11334    cutter subject biblical backing koreshs tape c...\n",
       "11335    subject albert sabin rich fox univ south dakot...\n",
       "Name: Content, Length: 11336, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Content'].apply(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
