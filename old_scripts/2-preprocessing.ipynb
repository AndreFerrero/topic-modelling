{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# run this cell if this package is not installed\n",
    "# %pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run 1-setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "from nltk.corpus import words, wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_word(word):\n",
    "    english_word_set = set(words.words())\n",
    "    return word.lower() in english_word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    wpt = WordPunctTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # we extend the stop words to avoid retaining based on the words that appear in the final model\n",
    "    stop_words.extend(['hi', 'thanks', 'lot', 'dont', 'article', 'everyone', 'anyone', 'someone', 'nothing',\n",
    "                       'something', 'anything', 'everybody', 'somebody', 'anybody', 'please', 'ask', 'mo', 'mon', 'eh', 'da', \n",
    "                       'th', 'dude', 'sh', 'ra', 'li', 'ce', 'people', 'university', 'dod',\n",
    "                       'question', 'yeah', 'shouldnt', 'theyre', 'thing', 'theyll', 'didnt', 'sorry', 'hey',\n",
    "                       'oh', 'thats', 'thank', 'cannot', 'right', 'would', 'one', 'get', 'know', 'like', 'use', 'go',\n",
    "                       'think', 'make', 'say', 'see', 'also', 'could', 'well', 'want', 'way', 'take', 'find', 'need', 'try',\n",
    "                       'much', 'come', 'many', 'may', 'give', 'really', 'tell', 'two', 'still', 'read', 'might', 'write',\n",
    "                       'never', 'look', 'sure', 'day', 'even', 'new', 'time', 'good', 'first', 'keep', 'since', 'last', \n",
    "                       'long', 'fact', 'must', 'cant', 'another', 'little', 'without'])\n",
    "    \n",
    "    # remove the names and surnames that appear in the final models\n",
    "    stop_words.extend(['kent', 'sandvik', 'spencer', 'peter', 'henry', 'jack', 'frank', 'smith',\n",
    "                       'larry', 'marc', 'miller', 'mike', 'jeff', 'martin', 'bob', 'alan', 'rob', ])\n",
    "    \n",
    "    # remove email addresses\n",
    "    doc = re.sub(r'\\b\\S*@\\S*\\.\\S*\\b', '', doc)\n",
    "    \n",
    "    # remove special characters and digits, retaining only words with letters\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "    \n",
    "    # lowercase and strip\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    # remove brackets of any kind\n",
    "    doc = re.sub(r'[(){}[\\]]', '', doc)\n",
    "    \n",
    "    # remove punctuation\n",
    "    doc = doc.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # retain only English words\n",
    "    doc = ' '.join(word for word in doc.split() if is_english_word(word))\n",
    "    \n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    \n",
    "    # determine POS of the tokens\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # map POS tags to WordNet POS tags\n",
    "    tag_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "\n",
    "    # lemmatize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, tag_map.get(pos[0], wordnet.NOUN)) for token, pos in pos_tags]\n",
    "    \n",
    "    # # keep only nouns\n",
    "    # lemmatized_tokens = [lemmatizer.lemmatize(token, wordnet.NOUN) for token, pos in pos_tags if pos.startswith('N')]\n",
    "    \n",
    "    # filter stopwords out of lemmatized tokens \n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    \n",
    "    # recreate the document\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_Content'] = df['Content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_Content'].iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty documents\n",
    "empty_documents = df[df['Clean_Content'].str.strip() == '']\n",
    "\n",
    "# Count the number of empty documents\n",
    "num_empty_documents = len(empty_documents)\n",
    "\n",
    "if num_empty_documents > 0:\n",
    "    print(f\"Number of empty documents: {num_empty_documents}\")\n",
    "    print(\"Indices of empty documents:\")\n",
    "    print(empty_documents.index)\n",
    "else:\n",
    "    print(\"No empty documents found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
