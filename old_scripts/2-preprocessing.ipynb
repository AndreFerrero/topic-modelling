{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# run this cell if this package is not installed\n",
    "# %pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run 1-setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "from nltk.corpus import words, wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_word(word):\n",
    "    english_word_set = set(words.words())\n",
    "    return word.lower() in english_word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # Remove email addresses\n",
    "    doc = re.sub(r'\\b\\S*@\\S*\\.\\S*\\b', '', doc)\n",
    "    \n",
    "    # Remove special characters and digits, retain only words with letters\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "    \n",
    "    # Lowercase and strip\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # Remove brackets of any kind\n",
    "    doc = re.sub(r'[(){}[\\]]', '', doc)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    doc = doc.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Tokenize document\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    # POS tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "     # map POS tags to WordNet POS tags\n",
    "    tag_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, tag_map.get(pos[0], wordnet.NOUN)) for token, pos in pos_tags]\n",
    "    \n",
    "    \n",
    "    # Filter stopwords out of lemmatized tokens\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    stop_words.extend(['hi', 'thanks', 'lot', 'dont', 'article', 'everyone', 'anyone', 'someone', 'nothing',\n",
    "                       'something', 'anything', 'everybody', 'somebody', 'anybody', 'please', 'ask', 'people', 'university',\n",
    "                       'question', 'yeah', 'shouldnt', 'theyre', 'thing', 'theyll', 'didnt', 'sorry', 'hey',\n",
    "                       'oh', 'thats', 'thank', 'cannot', 'right', 'would', 'one', 'get', 'know', 'like', 'use', 'go',\n",
    "                       'think', 'make', 'say', 'see', 'also', 'could', 'well', 'want', 'way', 'take', 'find', 'need', 'try',\n",
    "                       'much', 'come', 'many', 'may', 'give', 'really', 'tell', 'two', 'still', 'read', 'might', 'write',\n",
    "                       'never', 'look', 'sure', 'day', 'even', 'new', 'time', 'good', 'first', 'keep', 'since', 'last', \n",
    "                       'long', 'fact', 'must', 'cant', 'another', 'little', 'without', 'csutexasedu', 'nntppostinghost',\n",
    "                       'im', 'seem', 'replyto', 'let', 'group', 'call', 'seem', ])\n",
    "    \n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    \n",
    "    # Recreate the document\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_Content'] = df['Content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_Content'].iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty documents\n",
    "empty_documents = df[df['Clean_Content'].str.strip() == '']\n",
    "\n",
    "# Count the number of empty documents\n",
    "num_empty_documents = len(empty_documents)\n",
    "\n",
    "if num_empty_documents > 0:\n",
    "    print(f\"Number of empty documents: {num_empty_documents}\")\n",
    "    print(\"Indices of empty documents:\")\n",
    "    print(empty_documents.index)\n",
    "else:\n",
    "    print(\"No empty documents found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
